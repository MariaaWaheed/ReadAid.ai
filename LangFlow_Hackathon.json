{"id":"99f1b4ea-9b90-43bb-8956-7f28e8964e15","data":{"nodes":[{"id":"CustomComponent-Y4f4a","type":"genericNode","position":{"x":-5.665743286014731,"y":-20.577759023738594},"data":{"type":"TavilySearchComponent","node":{"template":{"_type":"Component","code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from langflow.custom import Component\r\nfrom langflow.inputs import MessageTextInput\r\nfrom langflow.template import Output\r\nimport os\r\nfrom langchain.utilities.tavily_search import TavilySearchAPIWrapper\r\nfrom langchain.agents import initialize_agent, AgentType\r\nfrom langchain_community.chat_models import ChatOpenAI\r\nfrom langchain.tools.tavily_search import TavilySearchResults\r\n\r\nclass TavilySearchComponent(Component):\r\n    icon = \"🔍\"  # You can choose an appropriate icon or emoji\r\n\r\n    inputs = [\r\n        MessageTextInput(\r\n            name=\"message\",\r\n            display_name=\"message\",\r\n            info=\"Query to search for.\",\r\n        ),\r\n    ]\r\n\r\n    outputs = [\r\n        Output(display_name=\"Search Results\", name=\"results\", method=\"process_query\"),\r\n    ]\r\n\r\n    def process_query(self) -> str:\r\n        message = self.message\r\n        \r\n        # Fetch API keys from environment variables\r\n        tavily_api_key = os.getenv(\"TAVILY_API_KEY\")\r\n        openai_api_key = os.getenv(\"OPENAI_API_KEY\")\r\n        \r\n        if not tavily_api_key or not openai_api_key:\r\n            raise ValueError(\"API keys not found in environment variables\")\r\n\r\n        # Initialize components\r\n        llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0.7)\r\n        search = TavilySearchAPIWrapper()\r\n        tavily_tool = TavilySearchResults(api_wrapper=search)\r\n        \r\n        # Initialize agent\r\n        agent_chain = initialize_agent(\r\n            [tavily_tool],\r\n            llm,\r\n            agent=AgentType.STRUCTURED_CHAT_ZERO_SHOT_REACT_DESCRIPTION,\r\n            verbose=True,\r\n        )\r\n\r\n        results = agent_chain.run(message + \". Explain in detail with points.\")\r\n        self.status = results\r\n        return results\r\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"message":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"message","display_name":"message","advanced":false,"input_types":["Message"],"dynamic":false,"info":"Query to search for.","title_case":false,"type":"str"}},"icon":"🔍","base_classes":["Text"],"display_name":"Tavily Search and ReAct Agent","documentation":"","custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Text"],"selected":"Text","name":"results","display_name":"Search Results","method":"process_query","value":"__UNDEFINED__","cache":true}],"field_order":["message"],"beta":false,"edited":true},"id":"CustomComponent-Y4f4a","display_name":"Tavily Search and ReAct Agent"},"selected":false,"width":384,"height":295,"positionAbsolute":{"x":-5.665743286014731,"y":-20.577759023738594},"dragging":false},{"id":"Prompt-hnmuJ","type":"genericNode","position":{"x":478.0330640848847,"y":-19.801896245945215},"data":{"description":"Create a prompt template with dynamic variables.","display_name":"Prompt","id":"Prompt-hnmuJ","node":{"template":{"_type":"Component","code":{"advanced":true,"dynamic":true,"fileTypes":[],"file_path":"","info":"","list":false,"load_from_db":false,"multiline":true,"name":"code","password":false,"placeholder":"","required":true,"show":true,"title_case":false,"type":"code","value":"from langflow.base.prompts.api_utils import process_prompt_template\nfrom langflow.custom import Component\nfrom langflow.io import Output, PromptInput\nfrom langflow.schema.message import Message\nfrom langflow.template.utils import update_template_values\n\n\nclass PromptComponent(Component):\n    display_name: str = \"Prompt\"\n    description: str = \"Create a prompt template with dynamic variables.\"\n    icon = \"prompts\"\n    trace_type = \"prompt\"\n    name = \"Prompt\"\n\n    inputs = [\n        PromptInput(name=\"template\", display_name=\"Template\"),\n    ]\n\n    outputs = [\n        Output(display_name=\"Prompt Message\", name=\"prompt\", method=\"build_prompt\"),\n    ]\n\n    async def build_prompt(\n        self,\n    ) -> Message:\n        prompt = await Message.from_template_and_variables(**self._attributes)\n        self.status = prompt.text\n        return prompt\n\n    def post_code_processing(self, new_build_config: dict, current_build_config: dict):\n        \"\"\"\n        This function is called after the code validation is done.\n        \"\"\"\n        frontend_node = super().post_code_processing(new_build_config, current_build_config)\n        template = frontend_node[\"template\"][\"template\"][\"value\"]\n        _ = process_prompt_template(\n            template=template,\n            name=\"template\",\n            custom_fields=frontend_node[\"custom_fields\"],\n            frontend_node_template=frontend_node[\"template\"],\n        )\n        # Now that template is updated, we need to grab any values that were set in the current_build_config\n        # and update the frontend_node with those values\n        update_template_values(new_template=frontend_node, previous_template=current_build_config[\"template\"])\n        return frontend_node\n"},"template":{"advanced":false,"display_name":"Template","dynamic":false,"info":"","list":false,"load_from_db":false,"name":"template","placeholder":"","required":false,"show":true,"title_case":false,"trace_as_input":true,"type":"prompt","value":"You will be given input content at the end and you have to Convert it into more readable and friendly format for Dyslexic and ADHD individuals, and then generate an HTML with suitable format. You will also be given guidelines in start which you should follow while generating. You should only write HTML in your output. \n    \nHere are some guidelines to follow when generating the HTML:\n\n    Font and Text Guidelines:\n    1. Use Simple, Clean Fonts: Fonts like Arial, Verdana, and Calibri benefit dyslexic and ADHD readers.\n    2. Adequate Font Size: Utilize larger font sizes (14-16 points or more) and ensure proper character spacing.\n    3. Simple Language: Use straightforward language and avoid overly complex sentences.\n    4. Avoid Long Blocks of Text: Break text into shorter paragraphs to prevent overwhelm and improve readability.\n    \n    Layout and Design Guidelines:\n    1. Clear Headings and Subheadings: Use well-defined headings and subheadings to help navigate the content easily.\n    2. Bullet Points and Lists: Utilize bullet points or numbered lists to present information in a structured and digestible manner.\n    3. Consistent Layout: A predictable and consistent layout aids in navigation and comprehension for both groups.\n   \n    Additional Formatting Guidelines:\n    1. Bold the first character at the beginning of each sentence.\n    2. Highlight the important words in sentences with \"blue color\".\n    3. The output of  should be in \"HTML format\".\n    Please keep in mind the above steps when generating the HTML.\n\n    \n   Following is the Input Content that you have to use: {user_input}.\n\n  Don't write any explanation. Don't ask any question. Don't write guidelines. Only give HTML output based on the Input Content."},"user_input":{"field_type":"str","required":false,"placeholder":"","list":false,"show":true,"multiline":true,"value":"","fileTypes":[],"file_path":"","password":false,"name":"user_input","display_name":"user_input","advanced":false,"input_types":["Message","Text"],"dynamic":false,"info":"","load_from_db":false,"title_case":false,"type":"str"}},"description":"Create a prompt template with dynamic variables.","icon":"prompts","is_input":null,"is_output":null,"is_composition":null,"base_classes":["Message"],"name":"","display_name":"Prompt","documentation":"","custom_fields":{"template":["user_input"]},"output_types":[],"full_path":null,"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Message"],"selected":"Message","name":"prompt","hidden":null,"display_name":"Prompt Message","method":"build_prompt","value":"__UNDEFINED__","cache":true}],"field_order":["template"],"beta":false,"error":null,"edited":false},"type":"Prompt"},"selected":false,"width":384,"height":421,"positionAbsolute":{"x":478.0330640848847,"y":-19.801896245945215},"dragging":false},{"id":"OpenAIModel-Nvh4f","type":"genericNode","position":{"x":968.5175248148935,"y":-24.164791759593797},"data":{"description":"Generates text using OpenAI LLMs.","display_name":"OpenAI","id":"OpenAIModel-Nvh4f","node":{"base_classes":["LanguageModel","Message"],"beta":false,"conditional_paths":[],"custom_fields":{},"description":"Generates text using OpenAI LLMs.","display_name":"OpenAI","documentation":"","edited":false,"field_order":["input_value","max_tokens","model_kwargs","json_mode","output_schema","model_name","openai_api_base","openai_api_key","temperature","stream","system_message","seed"],"frozen":false,"icon":"OpenAI","output_types":[],"outputs":[{"cache":true,"display_name":"Text","method":"text_response","name":"text_output","selected":"Message","types":["Message"],"value":"__UNDEFINED__"},{"cache":true,"display_name":"Language Model","method":"build_model","name":"model_output","selected":"LanguageModel","types":["LanguageModel"],"value":"__UNDEFINED__"}],"pinned":false,"template":{"_type":"Component","code":{"advanced":true,"dynamic":true,"fileTypes":[],"file_path":"","info":"","list":false,"load_from_db":false,"multiline":true,"name":"code","password":false,"placeholder":"","required":true,"show":true,"title_case":false,"type":"code","value":"import operator\nfrom functools import reduce\n\nfrom langchain_openai import ChatOpenAI\nfrom pydantic.v1 import SecretStr\n\nfrom langflow.base.constants import STREAM_INFO_TEXT\nfrom langflow.base.models.model import LCModelComponent\nfrom langflow.base.models.openai_constants import MODEL_NAMES\nfrom langflow.field_typing import LanguageModel\nfrom langflow.inputs import (\n    BoolInput,\n    DictInput,\n    DropdownInput,\n    FloatInput,\n    IntInput,\n    MessageInput,\n    SecretStrInput,\n    StrInput,\n)\n\n\nclass OpenAIModelComponent(LCModelComponent):\n    display_name = \"OpenAI\"\n    description = \"Generates text using OpenAI LLMs.\"\n    icon = \"OpenAI\"\n    name = \"OpenAIModel\"\n\n    inputs = [\n        MessageInput(name=\"input_value\", display_name=\"Input\"),\n        IntInput(\n            name=\"max_tokens\",\n            display_name=\"Max Tokens\",\n            advanced=True,\n            info=\"The maximum number of tokens to generate. Set to 0 for unlimited tokens.\",\n        ),\n        DictInput(name=\"model_kwargs\", display_name=\"Model Kwargs\", advanced=True),\n        BoolInput(\n            name=\"json_mode\",\n            display_name=\"JSON Mode\",\n            advanced=True,\n            info=\"If True, it will output JSON regardless of passing a schema.\",\n        ),\n        DictInput(\n            name=\"output_schema\",\n            is_list=True,\n            display_name=\"Schema\",\n            advanced=True,\n            info=\"The schema for the Output of the model. You must pass the word JSON in the prompt. If left blank, JSON mode will be disabled.\",\n        ),\n        DropdownInput(\n            name=\"model_name\", display_name=\"Model Name\", advanced=False, options=MODEL_NAMES, value=MODEL_NAMES[0]\n        ),\n        StrInput(\n            name=\"openai_api_base\",\n            display_name=\"OpenAI API Base\",\n            advanced=True,\n            info=\"The base URL of the OpenAI API. Defaults to https://api.openai.com/v1. You can change this to use other APIs like JinaChat, LocalAI and Prem.\",\n        ),\n        SecretStrInput(\n            name=\"openai_api_key\",\n            display_name=\"OpenAI API Key\",\n            info=\"The OpenAI API Key to use for the OpenAI model.\",\n            advanced=False,\n            value=\"OPENAI_API_KEY\",\n        ),\n        FloatInput(name=\"temperature\", display_name=\"Temperature\", value=0.1),\n        BoolInput(name=\"stream\", display_name=\"Stream\", info=STREAM_INFO_TEXT, advanced=True),\n        StrInput(\n            name=\"system_message\",\n            display_name=\"System Message\",\n            info=\"System message to pass to the model.\",\n            advanced=True,\n        ),\n        IntInput(\n            name=\"seed\",\n            display_name=\"Seed\",\n            info=\"The seed controls the reproducibility of the job.\",\n            advanced=True,\n            value=1,\n        ),\n    ]\n\n    def build_model(self) -> LanguageModel:  # type: ignore[type-var]\n        # self.output_schea is a list of dictionarie s\n        # let's convert it to a dictionary\n        output_schema_dict: dict[str, str] = reduce(operator.ior, self.output_schema or {}, {})\n        openai_api_key = self.openai_api_key\n        temperature = self.temperature\n        model_name: str = self.model_name\n        max_tokens = self.max_tokens\n        model_kwargs = self.model_kwargs or {}\n        openai_api_base = self.openai_api_base or \"https://api.openai.com/v1\"\n        json_mode = bool(output_schema_dict) or self.json_mode\n        seed = self.seed\n        model_kwargs[\"seed\"] = seed\n\n        if openai_api_key:\n            api_key = SecretStr(openai_api_key)\n        else:\n            api_key = None\n        output = ChatOpenAI(\n            max_tokens=max_tokens or None,\n            model_kwargs=model_kwargs,\n            model=model_name,\n            base_url=openai_api_base,\n            api_key=api_key,\n            temperature=temperature or 0.1,\n        )\n        if json_mode:\n            if output_schema_dict:\n                output = output.with_structured_output(schema=output_schema_dict, method=\"json_mode\")  # type: ignore\n            else:\n                output = output.bind(response_format={\"type\": \"json_object\"})  # type: ignore\n\n        return output  # type: ignore\n\n    def _get_exception_message(self, e: Exception):\n        \"\"\"\n        Get a message from an OpenAI exception.\n\n        Args:\n            exception (Exception): The exception to get the message from.\n\n        Returns:\n            str: The message from the exception.\n        \"\"\"\n\n        try:\n            from openai import BadRequestError\n        except ImportError:\n            return\n        if isinstance(e, BadRequestError):\n            message = e.body.get(\"message\")  # type: ignore\n            if message:\n                return message\n        return\n"},"input_value":{"advanced":false,"display_name":"Input","dynamic":false,"info":"","input_types":["Message"],"list":false,"load_from_db":false,"name":"input_value","placeholder":"","required":false,"show":true,"title_case":false,"trace_as_input":true,"trace_as_metadata":true,"type":"str","value":""},"json_mode":{"advanced":true,"display_name":"JSON Mode","dynamic":false,"info":"If True, it will output JSON regardless of passing a schema.","list":false,"name":"json_mode","placeholder":"","required":false,"show":true,"title_case":false,"trace_as_metadata":true,"type":"bool","value":false},"max_tokens":{"advanced":true,"display_name":"Max Tokens","dynamic":false,"info":"The maximum number of tokens to generate. Set to 0 for unlimited tokens.","list":false,"name":"max_tokens","placeholder":"","required":false,"show":true,"title_case":false,"trace_as_metadata":true,"type":"int","value":""},"model_kwargs":{"advanced":true,"display_name":"Model Kwargs","dynamic":false,"info":"","list":false,"name":"model_kwargs","placeholder":"","required":false,"show":true,"title_case":false,"trace_as_input":true,"type":"dict","value":{}},"model_name":{"advanced":false,"display_name":"Model Name","dynamic":false,"info":"","name":"model_name","options":["gpt-4o","gpt-4-turbo","gpt-4-turbo-preview","gpt-4","gpt-3.5-turbo","gpt-3.5-turbo-0125"],"placeholder":"","required":false,"show":true,"title_case":false,"trace_as_metadata":true,"type":"str","value":"gpt-4o"},"openai_api_base":{"advanced":true,"display_name":"OpenAI API Base","dynamic":false,"info":"The base URL of the OpenAI API. Defaults to https://api.openai.com/v1. You can change this to use other APIs like JinaChat, LocalAI and Prem.","list":false,"load_from_db":false,"name":"openai_api_base","placeholder":"","required":false,"show":true,"title_case":false,"trace_as_metadata":true,"type":"str","value":""},"openai_api_key":{"advanced":false,"display_name":"OpenAI API Key","dynamic":false,"info":"The OpenAI API Key to use for the OpenAI model.","input_types":[],"load_from_db":false,"name":"openai_api_key","password":true,"placeholder":"","required":false,"show":true,"title_case":false,"type":"str","value":""},"output_schema":{"advanced":true,"display_name":"Schema","dynamic":false,"info":"The schema for the Output of the model. You must pass the word JSON in the prompt. If left blank, JSON mode will be disabled.","list":true,"name":"output_schema","placeholder":"","required":false,"show":true,"title_case":false,"trace_as_input":true,"type":"dict","value":{}},"seed":{"advanced":true,"display_name":"Seed","dynamic":false,"info":"The seed controls the reproducibility of the job.","list":false,"name":"seed","placeholder":"","required":false,"show":true,"title_case":false,"trace_as_metadata":true,"type":"int","value":1},"stream":{"advanced":true,"display_name":"Stream","dynamic":false,"info":"Stream the response from the model. Streaming works only in Chat.","list":false,"name":"stream","placeholder":"","required":false,"show":true,"title_case":false,"trace_as_metadata":true,"type":"bool","value":false},"system_message":{"advanced":true,"display_name":"System Message","dynamic":false,"info":"System message to pass to the model.","list":false,"load_from_db":false,"name":"system_message","placeholder":"","required":false,"show":true,"title_case":false,"trace_as_metadata":true,"type":"str","value":""},"temperature":{"advanced":false,"display_name":"Temperature","dynamic":false,"info":"","list":false,"name":"temperature","placeholder":"","required":false,"show":true,"title_case":false,"trace_as_metadata":true,"type":"float","value":"0.6"}}},"type":"OpenAIModel"},"selected":false,"width":384,"height":619,"positionAbsolute":{"x":968.5175248148935,"y":-24.164791759593797},"dragging":false},{"id":"ChatOutput-TtcDq","type":"genericNode","position":{"x":1453.9570084698682,"y":-18.0524991530924},"data":{"type":"ChatOutput","node":{"template":{"_type":"Component","code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from langflow.base.io.chat import ChatComponent\nfrom langflow.inputs import BoolInput\nfrom langflow.io import DropdownInput, MessageTextInput, Output\nfrom langflow.memory import store_message\nfrom langflow.schema.message import Message\n\n\nclass ChatOutput(ChatComponent):\n    display_name = \"Chat Output\"\n    description = \"Display a chat message in the Playground.\"\n    icon = \"ChatOutput\"\n    name = \"ChatOutput\"\n\n    inputs = [\n        MessageTextInput(\n            name=\"input_value\",\n            display_name=\"Text\",\n            info=\"Message to be passed as output.\",\n        ),\n        BoolInput(\n            name=\"store_message\",\n            display_name=\"Store Messages\",\n            info=\"Store the message in the history.\",\n            value=True,\n            advanced=True,\n        ),\n        DropdownInput(\n            name=\"sender\",\n            display_name=\"Sender Type\",\n            options=[\"Machine\", \"User\"],\n            value=\"Machine\",\n            advanced=True,\n            info=\"Type of sender.\",\n        ),\n        MessageTextInput(\n            name=\"sender_name\", display_name=\"Sender Name\", info=\"Name of the sender.\", value=\"AI\", advanced=True\n        ),\n        MessageTextInput(\n            name=\"session_id\", display_name=\"Session ID\", info=\"Session ID for the message.\", advanced=True\n        ),\n        MessageTextInput(\n            name=\"data_template\",\n            display_name=\"Data Template\",\n            value=\"{text}\",\n            advanced=True,\n            info=\"Template to convert Data to Text. If left empty, it will be dynamically set to the Data's text key.\",\n        ),\n    ]\n    outputs = [\n        Output(display_name=\"Message\", name=\"message\", method=\"message_response\"),\n    ]\n\n    def message_response(self) -> Message:\n        message = Message(\n            text=self.input_value,\n            sender=self.sender,\n            sender_name=self.sender_name,\n            session_id=self.session_id,\n        )\n        if self.session_id and isinstance(message, Message) and isinstance(message.text, str):\n            store_message(\n                message,\n                flow_id=self.graph.flow_id,\n            )\n            self.message.value = message\n\n        self.status = message\n        return message\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"data_template":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"{text}","name":"data_template","display_name":"Data Template","advanced":true,"input_types":["Message"],"dynamic":false,"info":"Template to convert Data to Text. If left empty, it will be dynamically set to the Data's text key.","title_case":false,"type":"str"},"input_value":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"input_value","display_name":"Text","advanced":false,"input_types":["Message"],"dynamic":false,"info":"Message to be passed as output.","title_case":false,"type":"str"},"sender":{"trace_as_metadata":true,"options":["Machine","User"],"required":false,"placeholder":"","show":true,"value":"Machine","name":"sender","display_name":"Sender Type","advanced":true,"dynamic":false,"info":"Type of sender.","title_case":false,"type":"str"},"sender_name":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"AI","name":"sender_name","display_name":"Sender Name","advanced":true,"input_types":["Message"],"dynamic":false,"info":"Name of the sender.","title_case":false,"type":"str"},"session_id":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"session_id","display_name":"Session ID","advanced":true,"input_types":["Message"],"dynamic":false,"info":"Session ID for the message.","title_case":false,"type":"str"},"store_message":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"value":true,"name":"store_message","display_name":"Store Messages","advanced":true,"dynamic":false,"info":"Store the message in the history.","title_case":false,"type":"bool"}},"description":"Display a chat message in the Playground.","icon":"ChatOutput","base_classes":["Message"],"display_name":"Chat Output","documentation":"","custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Message"],"selected":"Message","name":"message","display_name":"Message","method":"message_response","value":"__UNDEFINED__","cache":true}],"field_order":["input_value","store_message","sender","sender_name","session_id","data_template"],"beta":false,"edited":false},"id":"ChatOutput-TtcDq"},"selected":false,"width":384,"height":307,"positionAbsolute":{"x":1453.9570084698682,"y":-18.0524991530924},"dragging":false},{"id":"ChatInput-54UkF","type":"genericNode","position":{"x":-475.7107765729202,"y":-9.826166167144919},"data":{"type":"ChatInput","node":{"template":{"_type":"Component","files":{"trace_as_metadata":true,"file_path":"","fileTypes":["txt","md","mdx","csv","json","yaml","yml","xml","html","htm","pdf","docx","py","sh","sql","js","ts","tsx","jpg","jpeg","png","bmp","image"],"list":true,"required":false,"placeholder":"","show":true,"value":"","name":"files","display_name":"Files","advanced":true,"dynamic":false,"info":"Files to be sent with the message.","title_case":false,"type":"file"},"code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from langflow.base.data.utils import IMG_FILE_TYPES, TEXT_FILE_TYPES\nfrom langflow.base.io.chat import ChatComponent\nfrom langflow.inputs import BoolInput\nfrom langflow.io import DropdownInput, FileInput, MessageTextInput, MultilineInput, Output\nfrom langflow.memory import store_message\nfrom langflow.schema.message import Message\n\n\nclass ChatInput(ChatComponent):\n    display_name = \"Chat Input\"\n    description = \"Get chat inputs from the Playground.\"\n    icon = \"ChatInput\"\n    name = \"ChatInput\"\n\n    inputs = [\n        MultilineInput(\n            name=\"input_value\",\n            display_name=\"Text\",\n            value=\"\",\n            info=\"Message to be passed as input.\",\n        ),\n        BoolInput(\n            name=\"store_message\",\n            display_name=\"Store Messages\",\n            info=\"Store the message in the history.\",\n            value=True,\n            advanced=True,\n        ),\n        DropdownInput(\n            name=\"sender\",\n            display_name=\"Sender Type\",\n            options=[\"Machine\", \"User\"],\n            value=\"User\",\n            info=\"Type of sender.\",\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"sender_name\",\n            display_name=\"Sender Name\",\n            info=\"Name of the sender.\",\n            value=\"User\",\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"session_id\", display_name=\"Session ID\", info=\"Session ID for the message.\", advanced=True\n        ),\n        FileInput(\n            name=\"files\",\n            display_name=\"Files\",\n            file_types=TEXT_FILE_TYPES + IMG_FILE_TYPES,\n            info=\"Files to be sent with the message.\",\n            advanced=True,\n            is_list=True,\n        ),\n    ]\n    outputs = [\n        Output(display_name=\"message\", name=\"message\", method=\"message_response\"),\n    ]\n\n    def message_response(self) -> Message:\n        message = Message(\n            text=self.input_value,\n            sender=self.sender,\n            sender_name=self.sender_name,\n            session_id=self.session_id,\n            files=self.files,\n        )\n\n        if self.session_id and isinstance(message, Message) and isinstance(message.text, str):\n            store_message(\n                message,\n                flow_id=self.graph.flow_id,\n            )\n            self.message.value = message\n\n        self.status = message\n        return message\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"input_value":{"trace_as_input":true,"multiline":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"input_value","display_name":"Text","advanced":false,"input_types":["Message"],"dynamic":false,"info":"Message to be passed as input.","title_case":false,"type":"str"},"sender":{"trace_as_metadata":true,"options":["Machine","User"],"required":false,"placeholder":"","show":true,"value":"User","name":"sender","display_name":"Sender Type","advanced":true,"dynamic":false,"info":"Type of sender.","title_case":false,"type":"str"},"sender_name":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"User","name":"sender_name","display_name":"Sender Name","advanced":true,"input_types":["Message"],"dynamic":false,"info":"Name of the sender.","title_case":false,"type":"str"},"session_id":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"session_id","display_name":"Session ID","advanced":true,"input_types":["Message"],"dynamic":false,"info":"Session ID for the message.","title_case":false,"type":"str"},"store_message":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"value":true,"name":"store_message","display_name":"Store Messages","advanced":true,"dynamic":false,"info":"Store the message in the history.","title_case":false,"type":"bool"}},"description":"Get chat inputs from the Playground.","icon":"ChatInput","base_classes":["Message"],"display_name":"Chat Input","documentation":"","custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Message"],"selected":"Message","name":"message","display_name":"message","method":"message_response","value":"__UNDEFINED__","cache":true}],"field_order":["input_value","store_message","sender","sender_name","session_id","files"],"beta":false,"edited":true},"id":"ChatInput-54UkF","description":"Get chat inputs from the Playground.","display_name":"Chat Input"},"selected":false,"width":384,"height":307,"positionAbsolute":{"x":-475.7107765729202,"y":-9.826166167144919},"dragging":false}],"edges":[{"source":"Prompt-hnmuJ","sourceHandle":"{œdataTypeœ:œPromptœ,œidœ:œPrompt-hnmuJœ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}","target":"OpenAIModel-Nvh4f","targetHandle":"{œfieldNameœ:œinput_valueœ,œidœ:œOpenAIModel-Nvh4fœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}","data":{"targetHandle":{"fieldName":"input_value","id":"OpenAIModel-Nvh4f","inputTypes":["Message"],"type":"str"},"sourceHandle":{"dataType":"Prompt","id":"Prompt-hnmuJ","name":"prompt","output_types":["Message"]}},"id":"reactflow__edge-Prompt-hnmuJ{œdataTypeœ:œPromptœ,œidœ:œPrompt-hnmuJœ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}-OpenAIModel-Nvh4f{œfieldNameœ:œinput_valueœ,œidœ:œOpenAIModel-Nvh4fœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}","className":""},{"source":"OpenAIModel-Nvh4f","sourceHandle":"{œdataTypeœ:œOpenAIModelœ,œidœ:œOpenAIModel-Nvh4fœ,œnameœ:œtext_outputœ,œoutput_typesœ:[œMessageœ]}","target":"ChatOutput-TtcDq","targetHandle":"{œfieldNameœ:œinput_valueœ,œidœ:œChatOutput-TtcDqœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}","data":{"targetHandle":{"fieldName":"input_value","id":"ChatOutput-TtcDq","inputTypes":["Message"],"type":"str"},"sourceHandle":{"dataType":"OpenAIModel","id":"OpenAIModel-Nvh4f","name":"text_output","output_types":["Message"]}},"id":"reactflow__edge-OpenAIModel-Nvh4f{œdataTypeœ:œOpenAIModelœ,œidœ:œOpenAIModel-Nvh4fœ,œnameœ:œtext_outputœ,œoutput_typesœ:[œMessageœ]}-ChatOutput-TtcDq{œfieldNameœ:œinput_valueœ,œidœ:œChatOutput-TtcDqœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}","className":""},{"source":"ChatInput-54UkF","sourceHandle":"{œdataTypeœ:œChatInputœ,œidœ:œChatInput-54UkFœ,œnameœ:œmessageœ,œoutput_typesœ:[œMessageœ]}","target":"CustomComponent-Y4f4a","targetHandle":"{œfieldNameœ:œmessageœ,œidœ:œCustomComponent-Y4f4aœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}","data":{"targetHandle":{"fieldName":"message","id":"CustomComponent-Y4f4a","inputTypes":["Message"],"type":"str"},"sourceHandle":{"dataType":"ChatInput","id":"ChatInput-54UkF","name":"message","output_types":["Message"]}},"id":"reactflow__edge-ChatInput-54UkF{œdataTypeœ:œChatInputœ,œidœ:œChatInput-54UkFœ,œnameœ:œmessageœ,œoutput_typesœ:[œMessageœ]}-CustomComponent-Y4f4a{œfieldNameœ:œmessageœ,œidœ:œCustomComponent-Y4f4aœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}","className":""},{"source":"CustomComponent-Y4f4a","sourceHandle":"{œdataTypeœ:œTavilySearchComponentœ,œidœ:œCustomComponent-Y4f4aœ,œnameœ:œresultsœ,œoutput_typesœ:[œTextœ]}","target":"Prompt-hnmuJ","targetHandle":"{œfieldNameœ:œuser_inputœ,œidœ:œPrompt-hnmuJœ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}","data":{"targetHandle":{"fieldName":"user_input","id":"Prompt-hnmuJ","inputTypes":["Message","Text"],"type":"str"},"sourceHandle":{"dataType":"TavilySearchComponent","id":"CustomComponent-Y4f4a","name":"results","output_types":["Text"]}},"id":"reactflow__edge-CustomComponent-Y4f4a{œdataTypeœ:œTavilySearchComponentœ,œidœ:œCustomComponent-Y4f4aœ,œnameœ:œresultsœ,œoutput_typesœ:[œTextœ]}-Prompt-hnmuJ{œfieldNameœ:œuser_inputœ,œidœ:œPrompt-hnmuJœ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}","className":""}],"viewport":{"x":212.63451805066325,"y":181.63481901940867,"zoom":0.3298769776932241}},"description":"LangFlow_Hackathon","name":"LangFlow_Hackathon","last_tested_version":"1.0.9","endpoint_name":null,"is_component":false}